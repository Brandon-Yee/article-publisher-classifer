# -*- coding: utf-8 -*-
"""
EE 511

Final Project
"""

import pandas as pd
import numpy as np
from numpy.random import default_rng

"""
# 
# https://towardsdatascience.com/3-simple-ways-to-handle-large-data-with-pandas-d9164a3c02c1
columns = ['Unnamed: 0', 'Unnamed: 0.1', 'date', 'year', 'month', 'day', 'author', 'title', 'article', 'url', 'section', 'publication']
use_cols = ['year', 'month', 'day', 'title', 'article', 'section', 'publication']
ignore_cols = ['Unnamed: 0', 'Unnamed: 0.1', 'date', 'author', 'title', 'article', 'section', 'publication']


chunksize = 1000

data_iter = pd.read_csv('./article_data.csv', chunksize=chunksize)

for data in data_iter:
    print(data)
    break
"""

MAX_ART_LENGTH = 28000
MAX_TITLE_LENGTH = 50


def save_data(train, val, test, trainpath='./train_data.csv', valpath='./val_data.csv', testpath='./test_data.csv'):
    train.to_csv(trainpath)
    val.to_csv(valpath)
    test.to_csv(testpath)


def load_data(trainpath='./train_data.csv', valpath='./val_data.csv', testpath='./test_data.csv'):
    """
    Loads training, validation, and test datasets from file. To be used with
    the dataframs generated by *generate_data_sets*.
    PARAMETERS:
    trainpath - string: path to the training set 
    valpath - string: path to the validation set
    testpath - string: path to the test set
    RETURNS:
    train - pandas dataframe: df with 5 columns for training
    val - pandas dataframe: df with 5 columns for validation
    test - pandas dataframe: df with 5 columns for test
    """

    train = pd.read_csv(trainpath)
    val = pd.read_csv(valpath)
    test = pd.read_csv(testpath)
    return train, val, test


# data = pd.read_csv(path, usecols=['publication', 'title', 'article'], nrows=400)
def generate_data_sets(path):
    """
    Generates training, validation and test dataframes with the data in random
    ordering.
    Note: the returned dataframes have an 'orig_index' column (separate from
          the actual index of the dataframe) that indicates the original row
          within the original raw data that it corresponds to.
    PARAMETERS:
    path - string: path to original data csv
    RETURNS:
    train - pandas dataframe: df with 4 columns for training
    val - pandas dataframe: df with 4 columns for validation
    test - pandas dataframe: df w/ 4 cols for test
    """

    rng = default_rng(seed=6)
    data = pd.read_csv(path, usecols=['publication', 'title', 'article'])

    data = remove_reut(data)
    # determine all indexes that are not nan
    any_nan = data.isna().any(axis=1, bool_only=True)
    nan_idx = np.nonzero(any_nan.values)[0]
    remain_idx = np.setdiff1d(np.arange(len(data)), nan_idx,
                              assume_unique=True)

    data['title'].loc[remain_idx] = data['title'].loc[remain_idx].str.split()
    data['article'].loc[remain_idx] = data['article'].loc[remain_idx].str.split()

    long_idx = np.where(data['title'].str.len() > (MAX_TITLE_LENGTH-2))[0]
    for each_idx in long_idx:
        data['title'].loc[each_idx] = data['title'].loc[each_idx][:MAX_TITLE_LENGTH-2]

    long_idx = np.where(data['article'].str.len() > (MAX_ART_LENGTH-2))[0]
    for each_idx in long_idx:
        data['article'].loc[each_idx] = data['article'].loc[each_idx][:MAX_ART_LENGTH-2]

    for each_idx in remain_idx:
        this_title = data['title'].loc[each_idx]
        this_art = data['article'].loc[each_idx]

        this_title.insert(0, '<s>')
        this_title.append('</s>')
        if len(this_title) < MAX_TITLE_LENGTH:
            diff = MAX_TITLE_LENGTH - len(this_title)
            extra = diff * ['</s>']
            this_title[len(this_title):MAX_TITLE_LENGTH] = extra

        this_art.insert(0, '<s>')
        this_art.append('</s>')
        if len(this_art) < MAX_ART_LENGTH:
            diff = MAX_ART_LENGTH - len(this_art)
            extra = diff * ['</s>']
            this_art[len(this_art):MAX_ART_LENGTH] = extra

    # named target classes (does not include 'Other')
    tgt_classes = [
        'Reuters',
        'TechCrunch',
        'Economist',
        'CNN',
        'CNBC',
        'Fox News',
        'Politico',
        'The New York Times',
        'Vox',
        'Business Insider'
        ]

    class_idxs = {}
    train_idxs = {}
    val_idxs = {}
    test_idxs = {}


    # determine indexes for each named class
    for each in tgt_classes:
        # for each class, select 16K for training, 2K for val, and 2K for test

        # all indexes for the given class
        this_class = np.nonzero((data['publication'] == each).values)[0]
        class_idxs[each] = np.intersect1d(remain_idx, this_class,
                                          assume_unique=True)
        # randomly shuffle the indexes
        rng.shuffle(class_idxs[each])
        # select the needed amount
        train_idxs[each] = class_idxs[each][:16000]
        val_idxs[each] = class_idxs[each][16000:18000]
        test_idxs[each] = class_idxs[each][18000:20000]

    # concatenate all of the named classes of each category
    # to aggregate the data set
    class_train_idx = np.concatenate([arr for arr in train_idxs.values()])
    class_val_idx = np.concatenate([arr for arr in val_idxs.values()])
    class_test_idx = np.concatenate([arr for arr in test_idxs.values()])

    # determine the non-named class indexes and label them 'Other
    all_class_idx = np.concatenate([arr for arr in class_idxs.values()])
    extra_idx = np.setdiff1d(remain_idx, all_class_idx, assume_unique=True)
    data['publication'].loc[extra_idx] = 'Other'

    # shuffle them and select 20K
    rng.shuffle(extra_idx)
    other_train_idx = extra_idx[:16000]
    other_val_idx = extra_idx[16000:18000]
    other_test_idx = extra_idx[18000:20000]

    # add in the 'Other' rows
    all_train_idx = np.concatenate((class_train_idx, other_train_idx))
    all_val_idx = np.concatenate((class_val_idx, other_val_idx))
    all_test_idx = np.concatenate((class_test_idx, other_test_idx))

    # shuffle the indexes so that the classes are not grouped together
    rng.shuffle(all_train_idx)
    rng.shuffle(all_val_idx)
    rng.shuffle(all_test_idx)

    train = data.loc[all_train_idx]
    val = data.loc[all_val_idx]
    test = data.loc[all_test_idx]

    # reset the indexes and adjust the name of the new original index col
    train.reset_index(inplace=True)
    val.reset_index(inplace=True)
    test.reset_index(inplace=True)
    train.rename(columns={'index': 'orig_index'}, inplace=True)
    val.rename(columns={'index': 'orig_index'}, inplace=True)
    test.rename(columns={'index': 'orig_index'}, inplace=True)

    return train, val, test


def remove_reut(df):
    df['article'] = df['article'].str.replace('^.*\(Reuters\) - ', '',
                                              regex=True)
    return df
